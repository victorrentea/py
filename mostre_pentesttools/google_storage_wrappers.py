"""
For Victor: This is part of the Cloud Scanner tool which searches for vulnerabilities specific for cloud environments
(Amazon Web Services, Google Cloud Platform etc.) using their APIs to perform operations.

Representations of Google Storage resources: buckets, objects (blobs), policies/ACLs and methods for scanning them.
Bucket names are globally unique in Google Storage
(not even two different projects cand have buckets with the same name).

References:
- Public API (also supports authentication, but I didn't get the same info as with the Python client)
https://cloud.google.com/storage/docs/json_api/v1/buckets#methods
- (Auth) Python Client
https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.bucket.Bucket
"""
import datetime
import logging
import os
from collections import namedtuple
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from pathlib import Path
from typing import ClassVar, Optional

import google.api_core.exceptions
import requests
from google.cloud import storage
from google.oauth2 import service_account
from urllib3.exceptions import HTTPError

from tools.core import utils_slack
from tools.core.utils_base import DEFAULT_USER_AGENT
from tools.core.utils_cfg import get_base_directory

logging.getLogger("google.auth").setLevel(logging.WARNING)

BUCKET_BASE_URL = "https://storage.googleapis.com/"
"""Base domain of Google Storage resources."""

BUCKET_API_ENDPOINT = f"{BUCKET_BASE_URL}storage/v1/b/"
"""Endpoint for API operations on a Google Storage bucket. Append the bucket name at the end."""
# Might also be https://www.googleapis.com/storage/v1/b/

DATE_FORMAT_STRING = "%Y-%m-%d %H:%M:%S"
"""Format string to which all datetimes will be converted."""

INTERESTING_FILES_WORDLIST_PATH = os.path.join(
    get_base_directory(), "tools/url_fuzzer/old_wordlists/common_configs_wordlist.txt"
)
"""Wordlist containing common filenames that will be searched for in a bucket."""

IamPolicyBinding = namedtuple("IamPolicyBinding", "role members")
"""Representation of an IAM policy binding: each holds a role granted to specified members."""

AclEntry = namedtuple("AclEntry", "entity role")
"""Representation of an ACL entry: a entity and a role."""


@dataclass(frozen=True)
class Blob:
    """Representation of an object contained in a bucket and its associated details."""

    filename: str
    """Name of the file."""

    url: str
    """(Google Storage API) URL where the object can be viewed in the browser."""

    time_created: str
    """Time when the object was created."""

    last_updated: str
    """Time when the object was last updated."""

    owner: str
    """Owner of the object, if applicable."""

    acl: list[AclEntry] = field(default_factory=list)
    """List of ACL entries of the object, if applicable."""

    AccessType: ClassVar = IntEnum("AccessType", "NONE READER OWNER")
    """
    Kind of access one might have on the object: reader / owner (writer) / no access.
    Must be consistent with the ACL role values returned by Google (READER/OWNER), else KeyError will be raised.
    """

    public_access_type: AccessType = AccessType.NONE
    """Type of access a public user has on the object."""


class RelevantBucketPerms(Enum):
    """Relevant permissions that a bucket might have, that may lead to vulnerabilities."""

    GET_BUCKET = "storage.buckets.get"
    """Can retrieve details about bucket."""
    GET_IAM_POLICY = "storage.buckets.getIamPolicy"
    """Can read IAM policy of bucket."""
    SET_IAM_POLICY = "storage.buckets.setIamPolicy"
    """Can modify IAM policy of bucket -> privilege escalation."""
    LIST_OBJECTS = "storage.objects.list"
    """Can list objects in a bucket -> readable."""
    GET_OBJECTS = "storage.objects.get"
    """Can read objects in a bucket -> readable."""
    CREATE_OBJECTS = "storage.objects.create"
    """Can create new objects -> writable."""
    DELETE_OBJECTS = "storage.objects.delete"
    """Can delete objects -> writable."""
    UPDATE_OBJECTS = "storage.objects.update"
    """Can update objects -> writable."""
    # These two can raise error if queried auth when bucket has uniform access
    GET_OBJECTS_IAM_POLICY = "storage.objects.getIamPolicy"
    """Can read IAM policy of bucket objects."""
    SET_OBJECTS_IAM_POLICY = "storage.objects.setIamPolicy"
    """Can modify IAM policy of bucket objects -> writable."""


# https://cloud.google.com/storage/docs/access-control/iam-permissions
ALL_POSSIBLE_BUCKET_PERMS = set(perm.value for perm in RelevantBucketPerms).union(
    {
        # Other possible perms
        "storage.buckets.update",
        "storage.buckets.delete",
        "storage.buckets.getObjectInsights",
        "storage.buckets.createTagBinding",
        "storage.buckets.listEffectiveTags",
        "storage.buckets.listTagBindings",
        "storage.buckets.deleteTagBinding",
        "storage.multipartUploads.create",
        "storage.multipartUploads.list",
        "storage.multipartUploads.listParts",
        "storage.multipartUploads.abort",
        # Not queried perms:
        # "Providing storage.buckets.list or storage.buckets.create or Storage Insights inventory report permissions
        # returns an error, as these permissions apply to projects instead of buckets."
    }
)
"""All possible permissions that a bucket might have, to be checked for."""


def http_get(url: str) -> Optional[requests.Response]:
    """Do a simple HTTP GET on the given URL and return the response."""

    try:
        resp = requests.get(
            url,
            timeout=5,
            verify=False,
            headers={"User-Agent": DEFAULT_USER_AGENT},
        )

        return resp

    except (requests.RequestException, HTTPError) as exc:
        logging.warning("requests exception to %s: %s", url, str(exc))

    return None


def _convert_rfc_3339_datetime_string(rfc_3339_datetime_str: str) -> str:
    """Convert a datetime string given in RFC 3339 format e.g. "2023-07-06T19:12:32.790Z" into DATE_FORMAT_STRING."""

    # Under Python 3.11, we have to replace the ending Z to be supported by fromisoformat()
    # https://stackoverflow.com/a/62769371
    datetime_object = datetime.datetime.fromisoformat(rfc_3339_datetime_str.replace("Z", "+00:00"))

    return datetime.datetime.strftime(datetime_object, DATE_FORMAT_STRING)


def _get_auth_bucket(bucket_name: str) -> Optional[storage.Bucket]:
    """
    Obtain the Python bucket instance using the authenticated client.
    It is done by performing a GET on the bucket, so it fails if we don't have this permission.
    """

    auth_credentials = service_account.Credentials.from_service_account_file(
        Path(__file__).with_name("cloud-scanner-391814-2d9a12cc4082.json")
    )
    auth_client = storage.Client(credentials=auth_credentials)
    if not auth_client:
        logging.warning("Could not init storage auth client")
        return None

    try:
        return auth_client.get_bucket(bucket_name)

    except google.api_core.exceptions.Forbidden as exc:
        logging.info("Could not get bucket object auth %s: %s", bucket_name, str(exc))

    except ValueError:
        logging.exception("Could not get bucket object auth: %s", bucket_name)

    return None


@dataclass
class BucketWrapper:
    """Wrapper over a bucket and its associated details."""

    name: str
    """Actual bucket name."""

    scan_task_id: int = -1
    """Tool task_id, used in Slack notifications messages."""

    initial_get_response: Optional[dict] = field(default=None)
    """HTTP response JSON from performing an initial unauth GET on the bucket with the public Google API."""

    location: tuple[str, str] = ("", "")
    """
    Tuple of (location, location_type) attributes of the bucket.
    E.g: (US, multi-region) / (US, dual-region) / (NORTHAMERICA-NORTHEAST1, region).
    """

    time_created: str = ""
    """Time when the bucket was created."""

    last_updated: str = ""
    """Time when the bucket was last updated."""

    project_number: str = ""
    """Project number which the bucket is part of."""

    auth_bucket: Optional[storage.Bucket] = field(init=False, repr=False)
    """Bucket instance got with the Python Google Storage auth client."""

    unauth_perms: set[str] = field(default_factory=set)
    """Set of obtained unauth permissions of the bucket."""

    auth_perms: set[str] = field(default_factory=set)
    """Set of obtained auth permissions of the bucket."""

    objects_count: Optional[int] = None
    """Number of objects in bucket, or None if it could not be determined."""

    interesting_files: list[Blob] = field(default_factory=list)
    """Juicy objects found in bucket."""

    iam_policy: list[IamPolicyBinding] = field(default_factory=list)
    """List of the IAM policy bindings of the bucket."""

    acl: list[AclEntry] = field(default_factory=list)
    """List of ACL entries of the bucket, if applicable."""

    default_object_acl: list[AclEntry] = field(default_factory=list)
    """List of defaultObjectACL entries of the bucket, if applicable."""

    uniform_bucket_level_access: Optional[bool] = None
    """Whether the bucket has this option configured, or None until we precisely determine its value."""
    logging_enabled: Optional[bool] = None
    """Whether the bucket has this option configured, or None until we precisely determine its value."""
    versioning_enabled: Optional[bool] = None
    """Whether the bucket has this option configured, or None until we precisely determine its value."""
    labels_added: Optional[bool] = None
    """Whether the bucket has this option configured, or None until we precisely determine its value."""
    lifecycle_configured: Optional[bool] = None
    """Whether the bucket has this option configured, or None until we precisely determine its value."""
    retention_policy_configured: Optional[bool] = None
    """Whether the bucket has this option configured, or None until we precisely determine its value."""
    custom_encryption: Optional[bool] = None
    """Whether the bucket has this option configured, or None until we precisely determine its value."""

    cors_config: Optional[list] = None
    """The CORS configuration of the bucket, or None until we precisely determine its value."""

    def __post_init__(self):
        """Using the bucket name, load the auth bucket object and store it in self.auth_bucket."""

        self.auth_bucket = _get_auth_bucket(self.name)

    @property
    def public_perms(self) -> set[str]:
        """
        Return all the obtained public perms of the bucket, both auth and unauth
        (granted to allUsers + allAuthenticatedUsers).
        """

        return self.unauth_perms.union(self.auth_perms)

    def __obtain_public_perms(self):
        """Obtain the public self.unauth_perms and self.auth_perms of the bucket."""

        uniform_access_error_perms = {
            RelevantBucketPerms.GET_OBJECTS_IAM_POLICY.value,
            RelevantBucketPerms.SET_OBJECTS_IAM_POLICY.value,
        }
        uniform_access_error_msg = (
            "Cannot test storage.objects.getIamPolicy or storage.objects.setIamPolicy"
            " on buckets with uniform bucket-level access enabled"
        )
        billing_account_disabled_error_msg = "The billing account for the owning project is disabled"

        perms_to_query = ALL_POSSIBLE_BUCKET_PERMS
        if self.uniform_bucket_level_access is True:
            perms_to_query -= uniform_access_error_perms

        def __obtain_unauth_perms(perms_to_query):
            """
            Retrieve the list of unauthenticated permissions of the bucket by making a request to the public Google API
            and store it in self.unauth_perms.
            """

            logging.debug("Obtaining unauth perms...")

            base_url = f"{BUCKET_API_ENDPOINT}{self.name}/iam/testPermissions?"

            url = base_url + "&".join(f"permissions={perm}" for perm in perms_to_query)
            resp = http_get(url)

            # If uniform access error, retry excluding the perms that raised the error
            if (
                resp is not None
                and resp.status_code == 400
                and resp.json().get("error", {}).get("message") == uniform_access_error_msg
            ):
                self.uniform_bucket_level_access = True

                perms_to_query -= uniform_access_error_perms
                url = base_url + "&".join(f"permissions={perm}" for perm in perms_to_query)
                resp = http_get(url)

            perms = resp.json().get("permissions", []) if resp else []

            # If the billing account of the bucket is disabled, the API operation for getting the perms won't work.
            # So, check for some permissions in another way (using a try-and-error approach)
            if (
                resp is not None
                and resp.status_code == 403
                and billing_account_disabled_error_msg in resp.json().get("error", {}).get("message")
            ):
                # If the initial GET on the bucket URL succeeded, assume it has GET_BUCKET permission
                if self.initial_get_response is not None:
                    perms.append(RelevantBucketPerms.GET_BUCKET.value)

                # Make a request to list the buckets objects to check for LIST_OBJECTS permission
                if http_get(f"{BUCKET_API_ENDPOINT}{self.name}/o"):
                    perms.append(RelevantBucketPerms.LIST_OBJECTS.value)

            self.unauth_perms = set(perms)
            logging.info("Obtained unauth perms: %s", self.unauth_perms)

        def __obtain_auth_perms(perms_to_query):
            """
            Retrieve the list of authenticated permissions of the bucket by using the Python client
            and store it in self.auth_perms.
            """

            if not self.auth_bucket:
                logging.debug("No auth bucket object")

                return

            logging.debug("Obtaining auth perms...")

            try:
                auth_perms = self.auth_bucket.test_iam_permissions(perms_to_query)

            except google.api_core.exceptions.BadRequest as exc:
                # If uniform access error, retry excluding the perms that raised the error
                if uniform_access_error_msg in str(exc):
                    self.uniform_bucket_level_access = True
                    perms_to_query -= uniform_access_error_perms
                    auth_perms = self.auth_bucket.test_iam_permissions(perms_to_query)

                else:
                    logging.exception("Unknown exception while getting auth perms")
                    auth_perms = []

            except google.api_core.exceptions.Forbidden as exc:
                if billing_account_disabled_error_msg in str(exc):
                    logging.warning(exc)

                else:
                    logging.exception("Unknown exception while getting auth perms")

                auth_perms = []

            self.auth_perms = set(auth_perms)
            logging.info("Obtained auth perms: %s", self.auth_perms)

        __obtain_unauth_perms(perms_to_query)
        __obtain_auth_perms(perms_to_query)

    def __obtain_extra_details(self):
        """Try to obtain the location, time_created, last_updated, project_number and store them in the attributes."""

        logging.debug("Obtaining extra details...")

        # Get location
        if self.initial_get_response is not None and self.initial_get_response.get("location"):
            self.location = (self.initial_get_response["location"], self.initial_get_response.get("locationType", ""))
            logging.debug("Location unauth: %s", self.location)

        elif self.auth_bucket and self.auth_bucket.location:  # can be None
            self.location = (self.auth_bucket.location, self.auth_bucket.location_type or "")
            logging.debug("Location auth: %s", self.auth_bucket.location)

        # Get time created
        if self.initial_get_response is not None and self.initial_get_response.get("timeCreated"):
            self.time_created = _convert_rfc_3339_datetime_string(self.initial_get_response["timeCreated"])
            logging.debug("Time created unauth: %s", self.time_created)

        elif self.auth_bucket and self.auth_bucket.time_created:  # can be None
            self.time_created = datetime.datetime.strftime(self.auth_bucket.time_created, DATE_FORMAT_STRING)
            logging.debug("Time created auth: %s", self.time_created)

        # Get last updated time
        # We cannot get it from the auth bucket object, it doesn't have this attribute :(
        if self.initial_get_response is not None and self.initial_get_response.get("updated"):
            self.last_updated = _convert_rfc_3339_datetime_string(self.initial_get_response["updated"])
            logging.debug("Time last_updated unauth: %s", self.last_updated)

        # Get project number
        if self.initial_get_response is not None and self.initial_get_response.get("projectNumber"):
            self.project_number = self.initial_get_response["projectNumber"]
            logging.debug("Project number unauth: %s", self.project_number)

        elif self.auth_bucket and self.auth_bucket.project_number:
            self.project_number = self.auth_bucket.project_number
            logging.debug("Project number auth: %s", self.project_number)

    def __check_access_control(self):
        """
        Access to a bucket can be:
        - uniform bucket-level (recommended): a single set of permissions on the bucket and its objects, or
        - fine-grained: access to objects can be granted through object ACLs.

        The function checks if bucket has uniform_bucket_level_access and stores it in the attribute.
        If it's not enabled, it also tries to get the ACL and defaultObjectAcl and stores them in the attributes.
        """

        logging.debug("Checking bucket access control type...")

        # First we need to get the uniformBucketLevelAccess dict. First try unauth, else auth. Dict structure:
        #  "iamConfiguration": {
        #     "publicAccessPrevention": string,
        #     "uniformBucketLevelAccess": {
        #     "enabled": boolean,
        #     "lockedTime": "datetime"
        #     },
        #  }
        if self.initial_get_response is not None and "iamConfiguration" in self.initial_get_response:
            uniform_bucket_level_access_dict = (
                self.initial_get_response["iamConfiguration"].get("uniformBucketLevelAccess") or {}
            )
        elif self.auth_bucket:
            uniform_bucket_level_access_dict = (
                self.auth_bucket.iam_configuration.get("uniformBucketLevelAccess")
                or self.auth_bucket.iam_configuration.get("bucketPolicyOnly")  # legacy name with the same functionality
                or {}
            )
        else:
            uniform_bucket_level_access_dict = {}

        self.uniform_bucket_level_access = uniform_bucket_level_access_dict.get("enabled")
        logging.debug("Got uniform_bucket_level_access: %s", self.uniform_bucket_level_access)

    def __check_configurations(self):
        """
        Check if bucket has the configurations enabled and store it in the corresponding attributes:
        logging_enabled, versioning_enabled, retention_policy_configured,
        labels_added, lifecycle_configured, custom_encryption, cors_config

        Also send Slack notifications for attributes that we don't know how they look like yet.
        """

        def __check_logging():
            """Check if bucket has logging enabled."""

            logging.debug("Checking logging...")

            if self.initial_get_response is not None:
                self.logging_enabled = bool(self.initial_get_response.get("logging"))  # check not empty

            elif self.auth_bucket:
                self.logging_enabled = bool(self.auth_bucket.get_logging())  # check not empty or None

            logging.debug("Logging enabled: %s", self.logging_enabled)

        def __check_versioning():
            """Check if bucket has versioning enabled."""

            logging.debug("Checking versioning...")

            if self.initial_get_response is not None:
                self.versioning_enabled = self.initial_get_response.get("versioning", {}).get("enabled") is True

            elif self.auth_bucket:
                self.versioning_enabled = self.auth_bucket.versioning_enabled

            logging.debug("Versioning enabled: %s", self.versioning_enabled)

        def __check_retention_policy():
            """Check if bucket has retention policy configured."""

            logging.debug("Checking retention policy...")

            # https://cloud.google.com/storage/docs/bucket-lock
            if self.initial_get_response is not None:
                # Check not empty
                self.retention_policy_configured = bool(self.initial_get_response.get("retentionPolicy"))

            elif self.auth_bucket:
                # Check not empty or None
                self.retention_policy_configured = bool(self.auth_bucket.retention_policy_effective_time)

            logging.debug("Got retention_policy_configured: %s", self.retention_policy_configured)

        def __check_labels():
            """Check if bucket has labels added."""

            logging.debug("Checking labels...")

            if self.initial_get_response is not None:
                self.labels_added = bool(self.initial_get_response.get("labels"))  # check not empty

            elif self.auth_bucket:
                self.labels_added = bool(self.auth_bucket.labels)  # check not empty dict

            logging.debug("Got labels added: %s", self.labels_added)

        def __check_lifecycle_rules():
            """Check if bucket has lifecycle rules configured."""

            logging.debug("Checking lifecycle configured...")

            if self.initial_get_response is not None:
                self.lifecycle_configured = bool(self.initial_get_response.get("lifecycle"))  # check not empty

            elif self.auth_bucket:
                # lifecycle_rules is a generator, check if it is empty
                try:
                    next(self.auth_bucket.lifecycle_rules)
                except StopIteration:
                    self.lifecycle_configured = False
                else:
                    self.lifecycle_configured = True

            logging.debug("Got lifecycle_configured: %s", self.lifecycle_configured)

        def __check_cors():
            """Retrieve bucket CORS configuration and store it in cors_config."""

            logging.debug("Retrieving CORS configuration...")

            if self.initial_get_response is not None:
                self.cors_config = self.initial_get_response.get("cors", [])

            elif self.auth_bucket:
                self.cors_config = self.auth_bucket.cors  # might be empty list

            logging.debug("CORS configuration: %s", self.cors_config)

        def __check_encryption():
            """Check if bucket has object encryption with a Customer-Managed Encryption Key (CMEK)."""

            logging.debug("Checking encryption...")

            bucket_encryption = None

            if self.initial_get_response is not None:
                bucket_encryption = self.initial_get_response.get("encryption")
                self.custom_encryption = bool(bucket_encryption)
                if bucket_encryption:
                    logging.info("Encryption found in unauth response: %s", bucket_encryption)

            elif self.auth_bucket:
                bucket_encryption = self.auth_bucket.default_kms_key_name
                self.custom_encryption = bool(bucket_encryption)
                if bucket_encryption:
                    logging.info("default_kms_key_name found in auth bucket object: %s", bucket_encryption)

            if bucket_encryption is not None:
                utils_slack.send_slack_notification(
                    f"Hi {utils_slack.SlackUserIds.DANIEL_BECHENEA.value} and {utils_slack.SlackUserIds.CARINA_DEACONU.value}!"
                    f" Encryption found for Google Storage bucket {self.name}: {bucket_encryption}\n"
                    f"<https://app.pentest-tools.com/adminx/?p=latest_tasks&task_id={self.scan_task_id}|"
                    f"Task ID: {self.scan_task_id}>.",
                    utils_slack.SlackWebhooks.OFFENSIVE_TOOLS_NOTIFICATIONS,
                )

            logging.debug("Encryption: %s", self.custom_encryption)

        __check_logging()
        __check_versioning()
        __check_retention_policy()
        __check_labels()
        __check_lifecycle_rules()
        __check_cors()
        __check_encryption()

    def __obtain_iam_policy(self):
        """
        Retrieve the bucket IAM policy, if we have GET_IAM_POLICY permission, and store it in self.iam_policy.
        First, try it unauthenticated using the public Google API, else with the authenticated client.
        Calling this method assumes we have obtained the public perms.

        IAM policy entries consist of: role, members (principals) and, optionally, a condition for the rule.
        Conditions are only applicable if uniform_bucket_level_access is enabled.
        Conditions are not allowed on public principals (allUsers or allAuthenticatedUsers).
        """

        bindings = []
        logging.debug("Obtaining IAM policy...")

        # First, check if we can get it unauth
        # https://cloud.google.com/storage/docs/json_api/v1/buckets/getIamPolicy
        if RelevantBucketPerms.GET_IAM_POLICY.value in self.unauth_perms:
            resp = http_get(f"{BUCKET_API_ENDPOINT}{self.name}/iam")
            if resp is not None:
                bindings = resp.json().get("bindings", [])
                logging.debug("Unauth IAM policy: %s", bindings)

        # Else, try to get it with the auth client
        # https://googleapis.dev/python/google-api-core/latest/iam.html#google.api_core.iam.Policy
        elif RelevantBucketPerms.GET_IAM_POLICY.value in self.auth_perms and self.auth_bucket:
            policy = self.auth_bucket.get_iam_policy()
            bindings = policy.bindings
            logging.debug("Auth IAM policy: %s", bindings)

        for binding in bindings:
            self.iam_policy.append(IamPolicyBinding(binding["role"], set(binding["members"])))

        logging.info("Final IAM policy: %s", self.iam_policy)

    def __obtain_acl(self):
        """
        Try to obtain ACL and defaultObjectAcl, if we have necessary perms,
        and store it in the corresponding attributes.
        Only applicable if bucket doesn't have uniform bucket-level access enabled.
        Calling this method assumes we have obtained the public perms.
        """

        def __obtain_acl_unauth():
            """Try to obtain the bucket ACL and defaultObjectAcl using the public Google API."""

            logging.debug("Obtaining unauth ACL...")

            if (
                RelevantBucketPerms.GET_BUCKET.value not in self.unauth_perms
                or RelevantBucketPerms.GET_IAM_POLICY.value not in self.unauth_perms
            ):
                logging.debug("Not enough perms to get unauth ACL")

                return

            resp = http_get(f"{BUCKET_API_ENDPOINT}{self.name}?projection=full")
            if resp:  # status code 200
                for entry in resp.json().get("acl", []):
                    self.acl.append(AclEntry(entry["entity"], entry["role"]))
                logging.debug("ACL from unauth: %s", self.acl)

                for entry in resp.json().get("defaultObjectAcl", []):
                    self.default_object_acl.append(AclEntry(entry["entity"], entry["role"]))
                logging.debug("Default object ACL from unauth: %s", self.default_object_acl)

        def __obtain_acl_auth():
            """Try to obtain the bucket ACL and defaultObjectAcl using the Python auth client."""

            logging.debug("Obtaining auth ACL...")

            if RelevantBucketPerms.GET_IAM_POLICY.value not in self.auth_perms or not self.auth_bucket:
                logging.debug("Not enough perms to get auth ACL")

                return

            # _ACLEntity objects can look like this:
            #   {'identifier': None, 'roles': {'OWNER'}, 'type': 'allUsers'}
            #   {'identifier': 'editors-443311630845', 'roles': {'OWNER'}, 'type': 'project'}
            # Usually, there is only one role, even though the key is named "roles".
            try:
                # Check if we haven't already obtained it (using the public API)
                if not self.acl:
                    for entity in self.auth_bucket.acl.get_entities():
                        for role in entity.roles:
                            self.acl.append(
                                AclEntry(entity.type + (f"-{entity.identifier}" if entity.identifier else ""), role)
                            )
                    logging.debug("ACL from auth: %s", self.acl)

                # Check if we haven't already obtained it (using the public API)
                if not self.default_object_acl:
                    for entity in self.auth_bucket.default_object_acl.get_entities():
                        for role in entity.roles:
                            self.default_object_acl.append(
                                AclEntry(entity.type + (f"-{entity.identifier}" if entity.identifier else ""), role)
                            )
                    logging.debug("Default object ACL from auth: %s", self.default_object_acl)

            except google.api_core.exceptions.BadRequest as exc:
                if "Cannot get legacy ACL for a bucket that has uniform bucket-level access" in str(exc):
                    self.uniform_bucket_level_access = True
                    logging.debug("Could not get auth ACL", exc_info=True)
                else:
                    logging.exception("Unexpected Google Storage exception while obtaining auth ACL")

        if self.uniform_bucket_level_access is True:
            logging.debug("Uniform bucket-level access; ACLs not applicable")

            return

        __obtain_acl_unauth()
        __obtain_acl_auth()

    def __search_for_interesting_files(self):
        """
        List bucket objects and search for interesting filenames from the predefined wordlist in the bucket.
        Store what we find in self.interesting_files. Also compute the self.objects_count.
        Also see if they have ACLs, and store them in self.interesting_files_acls.
        Owner and ACL fields are only applicable if not uniform_bucket_level_access and we have perms to get them.
        Calling this method assumes we have obtained the public perms.
        """

        public_entities = ("allUsers", "allAuthenticatedUsers")

        def __compute_public_perms_access_type() -> Blob.AccessType:
            """Compute the default access type of an object for a public user by looking at the public bucket perms."""

            # Should we also take into account defaultObjectAcl?
            # It is only applicable if uniform_bucket_level_access is False and we could obtain the defaultObjectAcl.
            # Theoretically it applies to new objects, while existing ones are overwritten with object ACLs.
            # Even when we could obtain defaultObjectAcl and could not obtain an object ACL,
            # we shouldn't rely on the defaultObjectAcl,
            # because the object ACL could have set a more restrictive role.

            # Check if public users have WRITE perms on an existing object
            if any(
                perm.value in self.public_perms
                for perm in (
                    RelevantBucketPerms.DELETE_OBJECTS,
                    RelevantBucketPerms.UPDATE_OBJECTS,
                    RelevantBucketPerms.SET_IAM_POLICY,
                )
            ):
                return Blob.AccessType.OWNER

            # Check if public users have READ perms on an existing object
            if RelevantBucketPerms.GET_OBJECTS.value in self.public_perms:
                return Blob.AccessType.READER

            return Blob.AccessType.NONE

        def __search_for_interesting_files_unauth():
            """
            List bucket objects, count them, search for interesting files and their ACLs
            using the public unauth Google API.
            """

            logging.debug("Searching for interesting files unauth...")

            if not RelevantBucketPerms.LIST_OBJECTS.value in self.unauth_perms:
                logging.debug("No perms to list objects unauth")

                return

            url = f"{BUCKET_API_ENDPOINT}{self.name}/o"
            # If uniformBucketLevelAccess is not enabled, we can also query the 'owner' and 'acl' fields.
            # Even if it is, or we don't have permission to get them, we won't get an error on this request.
            if self.uniform_bucket_level_access is not True:
                url += "?projection=full"

            resp = http_get(url)
            if resp is None or not resp.json().get("items"):
                logging.warning("Could not get bucket objects from response")

                return

            self.objects_count = 0
            for item in resp.json()["items"]:
                self.objects_count += 1

                if item["name"] not in interesting_files:
                    continue

                logging.info("Found interesting file: %s", item["name"])

                # See if object has ACL
                object_acl = []
                acl_public_access_type = None
                for entry in item.get("acl", []):
                    object_acl.append(AclEntry(entry["entity"], entry["role"]))

                    # Check if this entry gives access to public users
                    if entry["entity"] in public_entities:
                        # Update the access type if it is more permissive than before
                        # E.g. allUsers have AccessType.NONE, but allAuthenticatedUsers have AccessType.READ
                        acl_public_access_type = (
                            Blob.AccessType[entry["role"]]
                            if acl_public_access_type is None
                            else max(acl_public_access_type, Blob.AccessType[entry["role"]])
                        )

                public_perms_access_type = __compute_public_perms_access_type()
                self.interesting_files.append(
                    Blob(
                        item["name"],
                        item["selfLink"],
                        _convert_rfc_3339_datetime_string(item["timeCreated"]),
                        _convert_rfc_3339_datetime_string(item["updated"]),
                        # 'owner': {'entity': 'user-carina.deaconu@pentest-tools.com'}
                        item.get("owner", {}).get("entity", ""),
                        object_acl,
                        acl_public_access_type or public_perms_access_type,
                    )
                )

        def __search_for_interesting_files_auth():
            """
            List bucket objects, count them, search for interesting files and their ACLs
            using the auth Python client.
            """

            logging.debug("Searching for interesting files auth...")

            if not RelevantBucketPerms.LIST_OBJECTS.value in self.auth_perms or not self.auth_bucket:
                logging.debug("Not enough perms to list objects auth")

                return

            self.objects_count = 0
            for blob in self.auth_bucket.list_blobs():
                self.objects_count += 1

                if blob.name not in interesting_files:
                    continue

                logging.info("Found interesting file: %s", blob.name)

                # So far we only got blob.owner=None, let's see if we find something else
                if blob.owner is not None:
                    logging.info("Using Python client, blob.owner=%s for blob %s", blob.owner, blob.name)
                    utils_slack.send_slack_notification(
                        f"Hi {utils_slack.SlackUserIds.DANIEL_BECHENEA.value} and {utils_slack.SlackUserIds.CARINA_DEACONU.value}!"
                        f" Blob owner {blob.owner} is not None for blob {blob.name}"
                        f" of Google Storage bucket {self.name}.\n"
                        f"<https://app.pentest-tools.com/adminx/?p=latest_tasks&task_id={self.scan_task_id}|"
                        f"Task ID: {self.scan_task_id}>.",
                        utils_slack.SlackWebhooks.OFFENSIVE_TOOLS_NOTIFICATIONS,
                    )

                # See if blob has object ACL
                blob_acl = []
                acl_public_access_type = None
                if (
                    RelevantBucketPerms.GET_OBJECTS.value in self.auth_perms
                    and RelevantBucketPerms.GET_OBJECTS_IAM_POLICY.value in self.auth_perms
                ):
                    for entry in blob.acl:
                        blob_acl.append(AclEntry(entry.entity, entry.role))
                        # Check if this entry gives access to public users
                        if entry.entity in public_entities:
                            # Update the access type if it is more permissive
                            # E.g. allUsers have AccessType.NONE, but allAuthenticatedUsers have AccessType.READ
                            acl_public_access_type = (
                                Blob.AccessType[entry.role]
                                if acl_public_access_type is None
                                else max(acl_public_access_type, Blob.AccessType[entry.role])
                            )

                public_perms_access_type = __compute_public_perms_access_type()
                self.interesting_files.append(
                    Blob(
                        blob.name,
                        blob.self_link,
                        datetime.datetime.strftime(blob.time_created, DATE_FORMAT_STRING),
                        datetime.datetime.strftime(blob.updated, DATE_FORMAT_STRING),
                        "",
                        blob_acl,
                        acl_public_access_type or public_perms_access_type,
                    )
                )

        logging.debug("Searching for interesting files...")

        try:
            with open(INTERESTING_FILES_WORDLIST_PATH, "r", encoding="utf-8") as fin:
                interesting_files = [filename.strip() for filename in fin.readlines()]
        except OSError:
            logging.exception("Could not open interesting files wordlist %s", INTERESTING_FILES_WORDLIST_PATH)
            interesting_files = []
            # Continue execution; we might at least compute the objects_count.

        # First try unauth
        __search_for_interesting_files_unauth()

        # If still didn't get the objects, try auth
        if self.objects_count is None:
            __search_for_interesting_files_auth()

        logging.info("Interesting files: %s", self.interesting_files)
        logging.info("Objects count: %s", self.objects_count)

        # TODO If we don't have LIST_OBJECTS permission, we could make requests to the public Google API, to the
        # GET object endpoint with all filenames in the wordlist and check if they exist.

    def gather_information(self):
        """
        Gather information about the bucket: unauth and auth perms, IAM policy/ACL,
        interesting files, config checks.
        """

        self.__obtain_extra_details()
        self.__check_configurations()
        self.__check_access_control()
        self.__obtain_public_perms()
        # The following need to have the public permissions already obtained
        self.__obtain_acl()
        self.__obtain_iam_policy()
        self.__search_for_interesting_files()
